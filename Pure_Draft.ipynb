{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZo9vrlTCkNch0xO+SFIa7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishikeshvishnu/project-SSY340/blob/xiaoying/Pure_Draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch tensorflow pretty_midi mido"
      ],
      "metadata": {
        "id": "BNRGJjF3Tjxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKIkhjaBU2OK",
        "outputId": "3cbfb5f6-4e90-4d75-ab5a-2f05ec8835ea"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I_EpbwBsTeQg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import mido\n",
        "import glob\n",
        "from random import randint\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing Code\n",
        "\n",
        "def sample_data(seqs, lth, factor=6):\n",
        "    \"\"\"\n",
        "    Randomly samples sequences of length ~lth from an input set of sequences.\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for seq in seqs:\n",
        "        length = randint(lth - lth // factor, lth + lth // factor)\n",
        "        idx = randint(0, max(0, len(seq) - length))\n",
        "        data.append(seq[idx:idx+length])\n",
        "    return data\n",
        "\n",
        "def preprocess_midi_files(source_dir, length):\n",
        "    \"\"\"\n",
        "    Preprocess MIDI files and convert them into sequences.\n",
        "    \"\"\"\n",
        "    DATA = []\n",
        "    for file in glob.iglob(source_dir + '**/*.mid*', recursive=True):\n",
        "        try:\n",
        "            idx_list, event_list, _ = midi_parser(file)\n",
        "            DATA.append(idx_list)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file}: {e}\")\n",
        "\n",
        "    # Sample the data\n",
        "    DATA = sample_data(DATA, length)\n",
        "    return DATA\n"
      ],
      "metadata": {
        "id": "0RacW98aTmGz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer Code\n",
        "def midi_parser(fname=None):\n",
        "    \"\"\"\n",
        "    Translates a single-track MIDI file into a tokenized event vocabulary.\n",
        "    \"\"\"\n",
        "    if fname is not None:\n",
        "        mid = mido.MidiFile(fname)\n",
        "\n",
        "    index_list = []\n",
        "    event_list = []\n",
        "    delta_time = 0\n",
        "    for track in mid.tracks:\n",
        "        for msg in track:\n",
        "            delta_time += msg.time\n",
        "            if msg.type == \"note_on\":\n",
        "                idx = msg.note + 1\n",
        "                index_list.append(idx)\n",
        "            elif msg.type == \"note_off\":\n",
        "                idx = msg.note + 1 + 128\n",
        "                index_list.append(idx)\n",
        "\n",
        "    return torch.LongTensor(index_list), event_list, delta_time\n"
      ],
      "metadata": {
        "id": "4Xxk2hwJTvUw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# LSTM Model definition\n",
        "def create_lstm_model(input_shape, output_dim):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.LSTM(128, input_shape=input_shape, return_sequences=True),\n",
        "        layers.LSTM(128),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(output_dim, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Example input shape\n",
        "input_shape = (25, 3)\n",
        "output_dim = 128  # for pitch prediction\n",
        "lstm_model = create_lstm_model(input_shape, output_dim)\n",
        "lstm_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "-57LHt2STxSw",
        "outputId": "d4abb800-2b04-43bc-e8a5-9be79a70c689"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m67,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │         \u001b[38;5;34m131,584\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m232,192\u001b[0m (907.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">232,192</span> (907.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m232,192\u001b[0m (907.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">232,192</span> (907.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Transformer Model definition\n",
        "class MusicTransformer(nn.Module):\n",
        "    def __init__(self, d_model=128, num_layers=3, num_heads=8, d_ff=512, vocab_size=416):\n",
        "        super(MusicTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.decoder = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=d_ff),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.final_layer = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)\n",
        "        x = self.decoder(x, x, tgt_mask=mask)\n",
        "        return self.final_layer(x)\n",
        "\n",
        "# Create transformer model\n",
        "transformer_model = MusicTransformer()\n",
        "print(transformer_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAipNqOdT1mt",
        "outputId": "02077c10-daae-4196-dd36-f4aea93b0f12"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MusicTransformer(\n",
            "  (embedding): Embedding(416, 128)\n",
            "  (decoder): TransformerDecoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-2): 3 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (multihead_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
            "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        (dropout3): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (final_layer): Linear(in_features=128, out_features=416, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_dir = '/content/drive/MyDrive/maestro-v3.0.0'\n",
        "seq_length = 25  # Define sequence length\n",
        "\n",
        "# Use the preprocessing function to get the processed data\n",
        "preprocessed_data = preprocess_midi_files(source_dir, seq_length)\n",
        "\n",
        "# Create X_train and y_train\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for sequence in preprocessed_data:\n",
        "    for i in range(len(sequence) - seq_length):\n",
        "        X_train.append(sequence[i:i+seq_length])\n",
        "        y_train.append(sequence[i+seq_length])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n"
      ],
      "metadata": {
        "id": "YAcW251YVw2a"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the LSTM Model\n",
        "# Parameters need to be modified\n",
        "X_train = np.random.rand(1000, 25, 3)  # Dummy data\n",
        "y_train = np.random.randint(0, 128, (1000, 128))\n",
        "\n",
        "history = lstm_model.fit(X_train, y_train, epochs=20, batch_size=4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg0OucoWT4YX",
        "outputId": "7c2df0b0-1448-4aaf-ee93-ec540f45ea57"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.0166 - loss: 15628184.0000\n",
            "Epoch 2/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.0033 - loss: 15737880.0000\n",
            "Epoch 3/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.0078 - loss: 17325768.0000\n",
            "Epoch 4/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.0019 - loss: 19535542.0000\n",
            "Epoch 5/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.0033 - loss: 22073636.0000\n",
            "Epoch 6/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.0114 - loss: 24345094.0000\n",
            "Epoch 7/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 12ms/step - accuracy: 0.0088 - loss: 25879864.0000\n",
            "Epoch 8/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.0056 - loss: 27056686.0000\n",
            "Epoch 9/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.0094 - loss: 28615334.0000\n",
            "Epoch 10/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.0053 - loss: 30398974.0000\n",
            "Epoch 11/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.0057 - loss: 31225074.0000\n",
            "Epoch 12/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.0130 - loss: 33143366.0000\n",
            "Epoch 13/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.0078 - loss: 32552688.0000\n",
            "Epoch 14/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.0120 - loss: 35437044.0000\n",
            "Epoch 15/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.0074 - loss: 36103088.0000\n",
            "Epoch 16/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.0087 - loss: 37729840.0000\n",
            "Epoch 17/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.0073 - loss: 36969580.0000\n",
            "Epoch 18/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.0084 - loss: 38811668.0000\n",
            "Epoch 19/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.0198 - loss: 42172920.0000\n",
            "Epoch 20/20\n",
            "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.0085 - loss: 42069580.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MidiDataset(Dataset):\n",
        "    def __init__(self, sequences, seq_length):\n",
        "        # Filter sequences that are shorter than the seq_length\n",
        "        self.sequences = [seq for seq in sequences if len(seq) > seq_length]\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        # Calculate the total number of valid slices (those with sufficient length)\n",
        "        return sum(len(seq) - self.seq_length for seq in self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Find which sequence the index corresponds to\n",
        "        seq_idx, token_idx = self._find_sequence_and_token_idx(idx)\n",
        "\n",
        "        # Get input and target sequences from that specific sequence\n",
        "        input_seq = self.sequences[seq_idx][token_idx:token_idx+self.seq_length]\n",
        "        target_seq = self.sequences[seq_idx][token_idx+1:token_idx+self.seq_length+1]\n",
        "\n",
        "        # Convert to tensors\n",
        "        return torch.LongTensor(input_seq), torch.LongTensor(target_seq)\n",
        "\n",
        "    def _find_sequence_and_token_idx(self, idx):\n",
        "        \"\"\"\n",
        "        Finds the sequence index and the starting token index based on the global index.\n",
        "        \"\"\"\n",
        "        cumulative_length = 0\n",
        "        for i, seq in enumerate(self.sequences):\n",
        "            length = len(seq) - self.seq_length\n",
        "            if idx < cumulative_length + length:\n",
        "                return i, idx - cumulative_length\n",
        "            cumulative_length += length\n",
        "        raise IndexError(f\"Index {idx} out of range\")\n",
        "\n",
        "\n",
        "# Example usage with preprocessed_data\n",
        "seq_length = 25\n",
        "dataset = MidiDataset(preprocessed_data, seq_length)\n",
        "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
      ],
      "metadata": {
        "id": "4mWVqE7rV_XZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Transformer Model\n",
        "# Change the values\n",
        "optimizer = torch.optim.Adam(transformer_model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop for transformer\n",
        "for epoch in range(20):\n",
        "    transformer_model.train()\n",
        "    for batch in train_loader:\n",
        "        inputs, targets = batch\n",
        "        optimizer.zero_grad()\n",
        "        output = transformer_model(inputs)\n",
        "        loss = nn.CrossEntropyLoss()(output.view(-1, output.size(-1)), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3NCpzn5UCTD",
        "outputId": "99519c55-f521-4040-9537-d9b1c1f5f892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 3.5879385471343994\n",
            "Epoch 1: Loss = 3.3744726181030273\n",
            "Epoch 2: Loss = 3.294386386871338\n",
            "Epoch 3: Loss = 3.37375545501709\n",
            "Epoch 4: Loss = 3.7287027835845947\n",
            "Epoch 5: Loss = 3.3724241256713867\n",
            "Epoch 6: Loss = 3.7472317218780518\n",
            "Epoch 7: Loss = 3.6279358863830566\n",
            "Epoch 8: Loss = 3.727896213531494\n",
            "Epoch 9: Loss = 3.570587158203125\n",
            "Epoch 10: Loss = 3.4100406169891357\n",
            "Epoch 11: Loss = 3.0404720306396484\n",
            "Epoch 12: Loss = 3.6588287353515625\n",
            "Epoch 13: Loss = 3.4368104934692383\n",
            "Epoch 14: Loss = 3.530839204788208\n",
            "Epoch 15: Loss = 3.506779193878174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music_transformer(model, start_sequence, num_steps=100):\n",
        "    generated = start_sequence\n",
        "    for _ in range(num_steps):\n",
        "        output = model(generated)\n",
        "        next_step = torch.argmax(output, dim=-1)[-1]  # Get the last prediction\n",
        "        generated = torch.cat((generated, next_step.unsqueeze(0)), dim=0)\n",
        "    return generated\n"
      ],
      "metadata": {
        "id": "DKD0XNrHUMTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_music_lstm(model, start_sequence, num_steps=100):\n",
        "    generated = start_sequence\n",
        "    for _ in range(num_steps):\n",
        "        next_step = model.predict(generated[-25:])  # Predict using last 25 steps\n",
        "        generated = np.append(generated, next_step, axis=0)\n",
        "    return generated\n"
      ],
      "metadata": {
        "id": "tUJ7AYNUUOUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare loss curves\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "plt.plot(history.history['loss'], label='LSTM Loss')\n",
        "\n",
        "plt.title('Model Loss Comparison')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Compare generated music (just print some tokens or analyze)\n",
        "lstm_output = generate_music_lstm(lstm_model, np.random.rand(1, 25, 3))\n",
        "transformer_output = generate_music_transformer(transformer_model, torch.LongTensor([1]))\n",
        "\n",
        "print(f\"LSTM Output: {lstm_output[:10]}\")\n",
        "print(f\"Transformer Output: {transformer_output[:10]}\")\n"
      ],
      "metadata": {
        "id": "8WwtLkKAUP-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1toOdNFQjFG1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}