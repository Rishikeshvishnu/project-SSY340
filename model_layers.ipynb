{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b829e01-1810-4449-bc3a-879cbe3ced2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nthis notebook contains helper functions for the decoder-only transformer architecture\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "this notebook contains helper functions for the decoder-only transformer architecture\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b13b7a8-e3ea-41de-9408-543ac3f24800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f64d890-313b-4a61-bdf8-af7a06cc432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinusoidal_positional_encoding(max_len, dim_model, dim_op=3):\n",
    "    \"\"\"\n",
    "    Creates sinusoidal position embeddings for transformer inputs. Each position is encoded\n",
    "    using sine for even dimensions and cosine for odd dimensions at different frequencies.\n",
    "    \n",
    "    Args:\n",
    "        max_len (int): Maximum sequence length to generate positions for\n",
    "        dim_model (int): Size of each position encoding vector (must be even)\n",
    "        dim_op (int): Target output dimension, default 3 for (batch, seq_len, dim_model)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Position encodings of shape ([1,]*dim_op-2, max_len, dim_model)\n",
    "                     Ready for broadcasting when added to input embeddings\n",
    "    \"\"\"\n",
    "    if dim_model % 2 != 0:\n",
    "        raise ValueError(\"dim_model must be even to split dimensions between sin/cos\")\n",
    "        \n",
    "    # Create position and dimension indices\n",
    "    positions = torch.arange(max_len, dtype=torch.float)\n",
    "    dimensions = torch.arange(0, dim_model, 2, dtype=torch.float)\n",
    "    \n",
    "    # Calculate angle rates\n",
    "    angle_rates = 1 / (10000 ** (dimensions / dim_model))\n",
    "    \n",
    "    # Calculate angles by outer product\n",
    "    angles = positions.unsqueeze(1) * angle_rates.unsqueeze(0)\n",
    "    \n",
    "    # Apply sin/cos\n",
    "    encodings = torch.zeros(max_len, dim_model)\n",
    "    encodings[:, 0::2] = torch.sin(angles)\n",
    "    encodings[:, 1::2] = torch.cos(angles)\n",
    "    \n",
    "    # Reshape for broadcasting\n",
    "    shape = [1] * (dim_op-2) + [max_len, dim_model]\n",
    "    return encodings.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58a08141-def7-4f08-8eaa-2b8b471b541d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_attention_scores(scores):\n",
    "    \"\"\"\n",
    "    Shifts attention scores to align relative position information along diagonals.\n",
    "    Optimized implementation of the skewing mechanism for relative position scores.\n",
    "    \n",
    "    Args:\n",
    "        scores (torch.Tensor): Attention scores tensor of shape (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Shifted scores with relative positions aligned on diagonals\n",
    "    \"\"\"\n",
    "    # Add padding column\n",
    "    padded = F.pad(scores, [1, 0])\n",
    "    \n",
    "    # Reshape for diagonal alignment\n",
    "    batch_dims = scores.shape[:-2]\n",
    "    seq_len_q, seq_len_k = scores.shape[-2:]\n",
    "    \n",
    "    # Efficient reshape and slice\n",
    "    shifted = padded.view(*batch_dims, seq_len_q, seq_len_k + 1)\n",
    "    shifted = shifted[..., 1:]\n",
    "    \n",
    "    return shifted\n",
    "\n",
    "def relative_attention(query, key, value, rel_pos_emb=None, mask=None):\n",
    "    \"\"\"\n",
    "    Computes attention scores considering both content and relative positions.\n",
    "    Implementation of relative positional attention mechanism optimized for music modeling.\n",
    "    \n",
    "    Args:\n",
    "        query (torch.Tensor): Query matrix (..., seq_len_q, dim)\n",
    "        key (torch.Tensor): Key matrix (..., seq_len_k, dim)\n",
    "        value (torch.Tensor): Value matrix (..., seq_len_k, dim)\n",
    "        rel_pos_emb (torch.Tensor, optional): Relative position embeddings (seq_len_k, dim)\n",
    "        mask (torch.Tensor, optional): Attention mask, 1s indicate positions to mask\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Attention output of shape (..., seq_len_q, dim)\n",
    "    \"\"\"\n",
    "    # Content-based attention scores\n",
    "    content_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "    \n",
    "    # Add relative position scores if provided\n",
    "    if rel_pos_emb is not None:\n",
    "        position_scores = shift_attention_scores(\n",
    "            torch.matmul(query, rel_pos_emb.transpose(-1, -2))\n",
    "        )\n",
    "    else:\n",
    "        position_scores = torch.zeros_like(content_scores)\n",
    "    \n",
    "    # Combine and scale attention scores\n",
    "    scale = 1 / sqrt(query.size(-1))\n",
    "    attention_scores = (content_scores + position_scores) * scale\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        attention_scores = attention_scores.masked_fill(mask == 1, -1e9)\n",
    "    \n",
    "    # Compute attention weights and final output\n",
    "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "    return torch.matmul(attention_weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be90b51-0b19-45d5-a05c-795c03577be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadRelativeAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Attention with Relative Position Encoding optimized for music sequence modeling.\n",
    "    Each head learns different musical aspects while considering relative positions between tokens.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, num_heads: int, max_rel_dist: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension (must be divisible by num_heads)\n",
    "            num_heads: Number of attention heads\n",
    "            max_rel_dist: Maximum relative distance for position encoding\n",
    "            dropout: Dropout rate for attention weights\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\")\n",
    "            \n",
    "        # Core dimensions\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.scale = self.head_dim ** -0.5  # Pre-compute attention scale\n",
    "        \n",
    "        # Linear projections\n",
    "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)  # Combined Q,K,V projection\n",
    "        self.output = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Relative position embedding\n",
    "        self.rel_pos_embedding = nn.Embedding(max_rel_dist, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _split_heads(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Splits input tensor into multiple heads.\n",
    "        shape: (batch, seq_len, d_model) -> (batch, num_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        batch, seq_len, _ = x.shape\n",
    "        x = x.view(batch, seq_len, self.num_heads, self.head_dim)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def _get_rel_pos(self, seq_len: int, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes relative position embeddings for given sequence length.\n",
    "        \"\"\"\n",
    "        pos_ids = torch.arange(seq_len, device=device)\n",
    "        rel_pos = pos_ids.unsqueeze(1) - pos_ids.unsqueeze(0)\n",
    "        rel_pos = rel_pos.clamp(-self.rel_pos_embedding.num_embeddings + 1, 0)\n",
    "        return self.rel_pos_embedding(-rel_pos)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute multi-head relative attention.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, d_model)\n",
    "            mask: Optional attention mask\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project Q, K, V together for efficiency\n",
    "        qkv = self.qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(self._split_heads, qkv)  # (batch, num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # Get relative position encodings\n",
    "        rel_pos = self._get_rel_pos(seq_len, x.device)\n",
    "        rel_pos = self._split_heads(rel_pos).squeeze(0)  # (num_heads, seq_len, head_dim)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        content_scores = torch.matmul(q, k.transpose(-2, -1))\n",
    "        position_scores = torch.matmul(q, rel_pos.transpose(-2, -1))\n",
    "        position_scores = relative_attention.shift_attention_scores(position_scores)\n",
    "        \n",
    "        # Combine and scale attention scores\n",
    "        attention_scores = (content_scores + position_scores) * self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 1, -1e9)\n",
    "        \n",
    "        # Get attention weights and apply to values\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        attention = torch.matmul(attention_weights, v)\n",
    "        \n",
    "        # Combine heads and project output\n",
    "        attention = attention.transpose(1, 2).contiguous()\n",
    "        output = attention.view(batch_size, seq_len, self.d_model)\n",
    "        return self.output(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76edff2b-0650-415d-a7cf-bb40e03c12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed Forward Network with two linear transformations and ReLU activation.\n",
    "    Processes each position independently and identically.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_model: int, dim_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_model: Input and output dimension\n",
    "            dim_ff: Hidden dimension of the feed-forward network\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(dim_model, dim_ff),\n",
    "            nn.GELU(),  # GELU typically works better than ReLU for transformers\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, dim_model)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights using Kaiming initialization\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight)\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, dim_model)\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, seq_len, dim_model)\n",
    "        \"\"\"\n",
    "        return self.network(x)\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-LN Transformer Decoder Block with relative positional self-attention.\n",
    "    Architecture: LayerNorm -> Self-Attention -> Add & Norm -> FFN -> Add & Norm\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        dim_model: int,\n",
    "        num_heads: int,\n",
    "        dim_ff: int,\n",
    "        max_rel_dist: int,\n",
    "        dropout: float = 0.1,\n",
    "        eps: float = 1e-6\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim_model: Model's hidden dimension\n",
    "            num_heads: Number of attention heads\n",
    "            dim_ff: Feed-forward network's hidden dimension\n",
    "            max_rel_dist: Maximum relative distance for positional encoding\n",
    "            dropout: Dropout probability\n",
    "            eps: LayerNorm epsilon\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Main layers\n",
    "        self.self_attention = MultiHeadRelativeAttention(\n",
    "            d_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            max_rel_dist=max_rel_dist,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.feed_forward = FeedForward(\n",
    "            dim_model=dim_model,\n",
    "            dim_ff=dim_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Normalization and regularization\n",
    "        self.norm1 = nn.LayerNorm(dim_model, eps=eps)\n",
    "        self.norm2 = nn.LayerNorm(dim_model, eps=eps)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Save dimensions for potential future use\n",
    "        self.dim_model = dim_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through decoder block.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, dim_model)\n",
    "            mask: Optional attention mask tensor\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (batch, seq_len, dim_model)\n",
    "        \"\"\"\n",
    "        # Self-attention block with residual connection\n",
    "        normed = self.norm1(x)\n",
    "        attended = self.self_attention(normed, mask=mask)\n",
    "        residual1 = x + self.dropout(attended)\n",
    "        \n",
    "        # Feed-forward block with residual connection\n",
    "        normed = self.norm2(residual1)\n",
    "        transformed = self.feed_forward(normed)\n",
    "        output = residual1 + self.dropout(transformed)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97e514aa-24e9-4aec-9d03-ac635a6d5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(sequence: torch.Tensor, pad_token: int = 0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates mask for padding tokens in sequences.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Input tensor of shape (batch_size, seq_len)\n",
    "        pad_token: Token used for padding, default 0\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of shape (batch_size, 1, 1, seq_len) where 1s indicate padding positions\n",
    "    \"\"\"\n",
    "    # Create boolean mask for padding tokens (True where padding exists)\n",
    "    mask = (sequence == pad_token)\n",
    "    \n",
    "    # Add dimensions for broadcasting with attention scores\n",
    "    return mask.unsqueeze(1).unsqueeze(1).float()\n",
    "\n",
    "def create_causal_mask(seq_len: int, device: Optional[torch.device] = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates causal mask to prevent attention to future tokens.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length of sequence\n",
    "        device: Optional torch device for mask\n",
    "        \n",
    "    Returns:\n",
    "        Upper triangular mask of shape (1, 1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Create upper triangular mask\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1)\n",
    "    \n",
    "    # Add dimensions for broadcasting\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "    if device is not None:\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "    return mask\n",
    "\n",
    "def create_combined_mask(\n",
    "    sequence: torch.Tensor,\n",
    "    pad_token: int = 0,\n",
    "    causal: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates combined padding and causal mask.\n",
    "    \n",
    "    Args:\n",
    "        sequence: Input tensor of shape (batch_size, seq_len)\n",
    "        pad_token: Token used for padding\n",
    "        causal: Whether to include causal masking\n",
    "        \n",
    "    Returns:\n",
    "        Combined mask of shape (batch_size, 1, seq_len, seq_len)\n",
    "    \"\"\"\n",
    "    # Get padding mask\n",
    "    padding_mask = create_padding_mask(sequence, pad_token)\n",
    "    \n",
    "    if not causal:\n",
    "        return padding_mask\n",
    "    \n",
    "    # Get causal mask and combine\n",
    "    causal_mask = create_causal_mask(sequence.size(-1), sequence.device)\n",
    "    return torch.maximum(padding_mask, causal_mask)\n",
    "\n",
    "def apply_mask(\n",
    "    attention_scores: torch.Tensor,\n",
    "    mask: torch.Tensor,\n",
    "    fill_value: float = -1e9\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies mask to attention scores.\n",
    "    \n",
    "    Args:\n",
    "        attention_scores: Raw attention scores\n",
    "        mask: Boolean mask (1s indicate positions to mask)\n",
    "        fill_value: Value to use for masked positions\n",
    "        \n",
    "    Returns:\n",
    "        Masked attention scores\n",
    "    \"\"\"\n",
    "    return attention_scores.masked_fill(mask == 1, fill_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35669621-ffc0-445d-a155-d7fbfcb63450",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
